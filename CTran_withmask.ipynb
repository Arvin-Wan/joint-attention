{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# First step is to import the needed libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "import math\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using cuda. Good!\n"
     ]
    }
   ],
   "source": [
    "# in this section we define static values and variables for ease of access and testing\n",
    "_fn=\"final\" # file unique id for saving and loading models\n",
    "bert_base='./pretrain/bert-base-uncased/'\n",
    "bert_large='./pretrain/bert-large-uncased/'\n",
    "deberta_base = \"./pretrain/deberta-v3-large\"\n",
    "\n",
    "# snips_all = \"./dataset/atis-2.w-intent_all.iob\"\n",
    "# snips_train=\"./dataset/snips_train.iob\"\n",
    "snips_test=\"./dataset/snips_test.iob\"\n",
    "snips_train_dev = \"./dataset/snips_train_dev.iob\"\n",
    "\n",
    "atis_train_dev = \"./dataset/atis-2.w-intent_train_dev.iob\"\n",
    "# atis_all = \"./dataset/atis-2.w-intent_all.iob\"\n",
    "# atis_train=\"./dataset/atis-2.train.w-intent.iob\"\n",
    "atis_test=\"./dataset/atis-2.test.w-intent.iob\"\n",
    "#ENV variables directly affect the model's behaviour\n",
    "ENV_DATASET_TRAIN=atis_train_dev\n",
    "ENV_DATASET_TEST=atis_test\n",
    "# ENV_DATASET_PATH = atis_all\n",
    "\n",
    "ENV_EMBEDDING_SIZE=768  # dimention of embbeding, bertbase=768,bertlarge&elmo=1024\n",
    "ENV_BERT_ADDR=bert_base\n",
    "ENV_SEED=1331\n",
    "ENV_HIDDEN_SIZE=ENV_EMBEDDING_SIZE\n",
    "ENV_CNN_FILTERS=ENV_HIDDEN_SIZE // 4  #128\n",
    "ENV_CNN_KERNELS=4\n",
    "DEPTH = 4\n",
    "\n",
    "#these are related to training\n",
    "BATCH_SIZE=32\n",
    "LENGTH=60\n",
    "STEP_SIZE=50\n",
    "\n",
    "# you must use cuda to run this code. if this returns false, you can not proceed.\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "torch.manual_seed(ENV_SEED)\n",
    "if USE_CUDA:\n",
    "    print(\"You are using cuda. Good!\")\n",
    "    torch.cuda.manual_seed(ENV_SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    print('You are NOT using cuda! Some problems may occur.')\n",
    "np.random.seed(ENV_SEED)\n",
    "random.seed(ENV_SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "implement dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#this function converts tokens to ids and then to a tensor\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = list(map(lambda w: to_ix[w] if w in to_ix.keys() else to_ix[\"<UNK>\"], seq))\n",
    "    tensor = Variable(torch.LongTensor(idxs)).cuda() if USE_CUDA else Variable(torch.LongTensor(idxs))\n",
    "    return tensor\n",
    "# this function turns class text to id\n",
    "def prepare_intent(intent, to_ix):\n",
    "    idxs = to_ix[intent] if intent in to_ix.keys() else to_ix[\"UNKNOWN\"]\n",
    "    return idxs\n",
    "# converts numbers to <NUM> TAG\n",
    "def number_to_tag(txt):\n",
    "    return \"<NUM>\" if txt.isdecimal() else txt\n",
    "\n",
    "# Here we remove multiple spaces and punctuation which cause errors in tokenization for bert & elmo.\n",
    "def remove_punc(mlist):\n",
    "    mlist = [re.sub(\" +\",\" \",t.split(\"\\t\")[0][4:-4]) for t in mlist] # remove spaces down to 1\n",
    "    temp_train_tokens = []\n",
    "    # punct remove example:  play samuel-el jackson from 2009 - 2010 > play samuelel jackson from 2009 - 2010\n",
    "    for row in mlist:\n",
    "        tokens = row.split(\" \")\n",
    "        newtokens = []\n",
    "        for token in tokens:\n",
    "            newtoken = re.sub(r\"[.,'\\\"\\\\/\\-:&’—=–官方杂志¡…“”~%]\",r\"\",token) # remove punc\n",
    "            newtoken = re.sub(r\"[楽園追放�]\",r\"A\",newtoken)\n",
    "            newtokens.append(newtoken if len(token)>1 else token)\n",
    "        if newtokens[-1]==\"\":\n",
    "            newtokens.pop(-1)\n",
    "        if newtokens[0]==\"\":\n",
    "            newtokens.pop(0)\n",
    "        temp_train_tokens.append(\" \".join(newtokens))\n",
    "    return temp_train_tokens\n",
    "# this function returns the main tokens so that we can apply tagging on them. see original paper.\n",
    "def get_subtoken_mask(current_tokens,bert_tokenizer):\n",
    "    temp_mask = []\n",
    "    for i in current_tokens:\n",
    "        temp_row_mask = []\n",
    "        temp_row_mask.append(False) # for cls token\n",
    "        temp = bert_tokenizer.tokenize(i)\n",
    "        for j in temp:\n",
    "            temp_row_mask.append(j[:2]!=\"##\")\n",
    "        while len(temp_row_mask)<LENGTH:\n",
    "            temp_row_mask.append(False)\n",
    "        temp_mask.append(temp_row_mask)\n",
    "        if sum(temp_row_mask)!=len(i.split(\" \")):\n",
    "            print(f\"inconsistent:{temp}\")\n",
    "            print(i)\n",
    "            print(sum(temp_row_mask))\n",
    "            print(len(i.split(\" \")))\n",
    "    return torch.tensor(temp_mask).cuda()\n",
    "\n",
    "flatten = lambda l: [number_to_tag(item) for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data load and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example input:BOS i want to fly from baltimore to dallas round trip EOS\tO O O O O O B-fromloc.city_name O B-toloc.city_name B-round_trip I-round_trip atis_flight\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiongcw/y/envs/temp/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/xiongcw/y/envs/temp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example input:BOS i would like to find a flight from charlotte to las vegas that makes a stop in st. louis EOS\tO O O O O O O O O B-fromloc.city_name O B-toloc.city_name I-toloc.city_name O O O O O B-stoploc.city_name I-stoploc.city_name atis_flight\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# def read_dataset_and_split_train_test(dataset_path):\n",
    "#     dataset = open(dataset_path,\"r\").readlines()\n",
    "#     print(\"example input:\"+dataset[0])\n",
    "#     dataset = [t[:-1] for t in dataset]\n",
    "#     train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=ENV_SEED, shuffle=True)\n",
    "#     return train_data, test_data\n",
    "\n",
    "\n",
    "# def tokenize_dataset(bert_tokenizer, dataset):\n",
    "#     #converts string to array of tokens + array of tags + target intent [array with x=3 and y dynamic]\n",
    "#     dataset_tokens = remove_punc(dataset)\n",
    "#     dataset_subtoken_mask = get_subtoken_mask(dataset_tokens,bert_tokenizer)\n",
    "#     dataset_toks = bert_tokenizer.batch_encode_plus(dataset_tokens,max_length=LENGTH,add_special_tokens=True,return_tensors='pt'\n",
    "#                                                   ,return_attention_mask=True , padding='max_length',truncation=True)\n",
    "#     dataset = [[re.sub(\" +\",\" \",t.split(\"\\t\")[0]).split(\" \"),t.split(\"\\t\")[1].split(\" \")[:-1],t.split(\"\\t\")[1].split(\" \")[-1]] for t in dataset]\n",
    "#     #removes BOS, EOS from array of tokens and tags\n",
    "#     dataset = [[t[0][1:-1],t[1][1:],t[2]] for t in dataset]\n",
    "#     return dataset, dataset_subtoken_mask,dataset_toks\n",
    "\n",
    "\n",
    "# train_data,test_data = read_dataset_and_split_train_test(ENV_DATASET_PATH)\n",
    "# from transformers import AutoTokenizer\n",
    "# bert_tokenizer = AutoTokenizer.from_pretrained(ENV_BERT_ADDR)\n",
    "\n",
    "# train,train_subtoken_mask,train_toks = tokenize_dataset(bert_tokenizer,train_data)\n",
    "# test, test_subtoken_mask, test_toks = tokenize_dataset(bert_tokenizer, test_data)\n",
    "\n",
    "def tokenize_dataset(dataset_address):\n",
    "    # added tokenizer and tokens for\n",
    "    from transformers import AutoTokenizer\n",
    "    bert_tokenizer = AutoTokenizer.from_pretrained(ENV_BERT_ADDR)\n",
    "    # bert_tokenizer = torch.hub.load(ENV_BERT_ADDR, 'tokenizer', ENV_BERT_ADDR,verbose=False,source=\"local\")#38toks snips,52Atis\n",
    "    ##open database and read line by line\n",
    "    dataset = open(dataset_address,\"r\").readlines()\n",
    "    print(\"example input:\"+dataset[0])\n",
    "    ##remove last character of lines -\\n- in train file\n",
    "    dataset = [t[:-1] for t in dataset]\n",
    "    #converts string to array of tokens + array of tags + target intent [array with x=3 and y dynamic]\n",
    "    dataset_tokens = remove_punc(dataset)\n",
    "    dataset_subtoken_mask = get_subtoken_mask(dataset_tokens,bert_tokenizer)\n",
    "    dataset_toks = bert_tokenizer.batch_encode_plus(dataset_tokens,max_length=LENGTH,add_special_tokens=True,return_tensors='pt'\n",
    "                                                  ,return_attention_mask=True , padding='max_length',truncation=True)\n",
    "    dataset = [[re.sub(\" +\",\" \",t.split(\"\\t\")[0]).split(\" \"),t.split(\"\\t\")[1].split(\" \")[:-1],t.split(\"\\t\")[1].split(\" \")[-1]] for t in dataset]\n",
    "    #removes BOS, EOS from array of tokens and tags\n",
    "    dataset = [[t[0][1:-1],t[1][1:],t[2]] for t in dataset]\n",
    "    return dataset, dataset_subtoken_mask,dataset_toks\n",
    "train,train_subtoken_mask,train_toks = tokenize_dataset(ENV_DATASET_TRAIN)\n",
    "test, test_subtoken_mask, test_toks = tokenize_dataset(ENV_DATASET_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#convert above array to separate lists\n",
    "seq_in,seq_out, intent = list(zip(*train))\n",
    "seq_in_test,seq_out_test, intent_test = list(zip(*test.copy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create Sets of unique tokens\n",
    "vocab = set(flatten(seq_in))           #len = 722\n",
    "slot_tag = set(flatten(seq_out))       # len = 120\n",
    "intent_tag = set(intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# adds paddings\n",
    "sin=[] #padded input tokens\n",
    "sout=[] # padded output translated tags\n",
    "sin_test=[] #padded input tokens\n",
    "sout_test=[] # padded output translated tags\n",
    "## adds padding inside input tokens\n",
    "def add_paddings(seq_in,seq_out):\n",
    "    sin=[]\n",
    "    sout=[]\n",
    "    for i in range(len(seq_in)):\n",
    "        temp = seq_in[i]\n",
    "        if len(temp)<LENGTH:\n",
    "            while len(temp)<LENGTH:\n",
    "                temp.append('<PAD>')\n",
    "        else:\n",
    "            temp = temp[:LENGTH]\n",
    "        temp = ['[cls]'] + temp[:LENGTH-1]\n",
    "        sin.append(temp)\n",
    "        # add padding inside output tokens\n",
    "        temp = seq_out[i]\n",
    "        if len(temp)<LENGTH:\n",
    "            while len(temp)<LENGTH:\n",
    "                temp.append('<PAD>')\n",
    "        else:\n",
    "            temp = temp[:LENGTH]\n",
    "        temp = ['O'] + temp[:LENGTH-1]\n",
    "        sout.append(temp)\n",
    "    return sin,sout\n",
    "sin,sout=add_paddings(seq_in,seq_out)\n",
    "sin_test,sout_test=add_paddings(seq_in_test,seq_out_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# making dictionary (token:id), initial value\n",
    "word2index = {'<PAD>': 0, '<UNK>':1,'<BOS>':2,'<EOS>':3,'<NUM>':4,'[cls]':5}\n",
    "# add rest of token list to dictionary\n",
    "for token in vocab:\n",
    "    if token not in word2index.keys():\n",
    "        word2index[token]=len(word2index)\n",
    "#make id to token list ( reverse )\n",
    "index2word = {v:k for k,v in word2index.items()}\n",
    "\n",
    "# initial tag2index dictionary\n",
    "tag2index = {'<PAD>' : 0,'<BOS>':2,'<UNK>':1,'<EOS>':3,'[cls]':4}\n",
    "# add rest of tag tokens to list\n",
    "for tag in slot_tag:\n",
    "    if tag not in tag2index.keys():\n",
    "        tag2index[tag] = len(tag2index)\n",
    "# making index to tag\n",
    "index2tag = {v:k for k,v in tag2index.items()}\n",
    "\n",
    "#initialize intent to index\n",
    "intent2index={'UNKNOWN':0}\n",
    "for ii in intent_tag:\n",
    "    if ii not in intent2index.keys():\n",
    "        intent2index[ii] = len(intent2index)\n",
    "index2intent = {v:k for k,v in intent2index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Loading PreTrained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#defining datasets.\n",
    "def remove_values_from_list(the_list, val):\n",
    "   return [value for value in the_list if value != val]\n",
    "\n",
    "class NLUDataset(Dataset):\n",
    "    def __init__(self, sin,sout,intent,input_ids,attention_mask,token_type_ids,subtoken_mask):\n",
    "        self.sin = [prepare_sequence(temp,word2index) for temp in sin]\n",
    "        self.sout = [prepare_sequence(temp,tag2index) for temp in sout]\n",
    "        self.intent = Variable(torch.LongTensor([prepare_intent(temp,intent2index) for temp in intent])).cuda()\n",
    "        self.input_ids=input_ids.cuda()\n",
    "        self.attention_mask=attention_mask.cuda()\n",
    "        self.token_type_ids=token_type_ids.cuda()\n",
    "        self.subtoken_mask=subtoken_mask.cuda()\n",
    "        self.x_mask = [Variable(torch.BoolTensor(tuple(map(lambda s: s ==0, t )))).cuda() for t in self.sin]\n",
    "    def __len__(self):\n",
    "        return len(self.intent)\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.sin[idx],self.sout[idx],self.intent[idx],self.input_ids[idx],self.attention_mask[idx],self.token_type_ids[idx],self.subtoken_mask[idx],self.x_mask[idx]\n",
    "        return sample\n",
    "#making single list\n",
    "train_data=NLUDataset(sin,sout,intent,train_toks['input_ids'],train_toks['attention_mask'],train_toks['token_type_ids'],train_subtoken_mask)\n",
    "test_data=NLUDataset(sin_test,sout_test,intent_test,test_toks['input_ids'],test_toks['attention_mask'],test_toks['token_type_ids'],test_subtoken_mask)\n",
    "train_data = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_data = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# we put all tags inside of the batch in a flat array for F1 measure.\n",
    "# we use masking so that we only non PAD tokens are counted in f1 measurement\n",
    "def mask_important_tags(predictions,tags,masks):\n",
    "    result_tags=[]\n",
    "    result_preds=[]\n",
    "    for pred,tag,mask in zip(predictions.tolist(),tags.tolist(),masks.tolist()):\n",
    "        #index [0] is to get the data\n",
    "        for p,t,m in zip(pred,tag,mask):\n",
    "            if not m:\n",
    "                result_tags.append(p)\n",
    "                result_preds.append(t)\n",
    "        #result_tags.pop()\n",
    "        #result_preds.pop()\n",
    "    return result_preds,result_tags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# generates transformer mask\n",
    "def generate_square_subsequent_mask(sz: int) :\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "def generate_square_diagonal_mask(sz: int) :\n",
    "    \"\"\"Generates a matrix which there are zeros on diag and other indexes are -inf.\"\"\"\n",
    "    return torch.triu(torch.ones(sz,sz)-float('inf'), diagonal=1)+torch.tril(torch.ones(sz,sz)-float('inf'), diagonal=-1)\n",
    "# positional embedding used in transformers\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        \"\"\"\n",
    "        LlamaRMSNorm is equivalent to T5LayerNorm\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.weight * hidden_states.to(input_dtype)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n",
    "\n",
    "#start of the shared encoder\n",
    "class BertLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertLayer, self).__init__()\n",
    "        from transformers import BertModel,AutoModel\n",
    "        # self.bert_model = torch.hub.load(ENV_BERT_ADDR, 'model', ENV_BERT_ADDR,source=\"local\")\n",
    "        self.bert_model = BertModel.from_pretrained(ENV_BERT_ADDR)\n",
    "\n",
    "    def forward(self, bert_info=None):\n",
    "        (bert_tokens, bert_mask, bert_tok_typeid) = bert_info\n",
    "        bert_encodings = self.bert_model(bert_tokens, bert_mask, bert_tok_typeid)\n",
    "        bert_last_hidden = bert_encodings['last_hidden_state']\n",
    "        bert_pooler_output = bert_encodings['pooler_output']\n",
    "        return bert_last_hidden, bert_pooler_output\n",
    "    \n",
    "class SENET(nn.Module):\n",
    "    def __init__(self, input_dim, ratio=4,p_dropout = 0.5):\n",
    "        super(SENET,self).__init__()\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.linear1 = nn.Linear(input_dim, input_dim // ratio)\n",
    "        self.linear2 = nn.Linear(input_dim // ratio, input_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(p_dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        batch_size, seq_len, dim = x.size()\n",
    "        \n",
    "        x = torch.transpose(x,dim0=1,dim1=2)\n",
    "        x1 = x\n",
    "        x1 = self.avgpool(x1).view(batch_size, dim)\n",
    "        x1 = self.activation(self.linear1(x1))\n",
    "        x1 = self.dropout(x1)\n",
    "        x1 = self.sigmoid(self.linear2(x1)).view(batch_size, dim, 1)\n",
    "        x = x * x1\n",
    "        x = torch.transpose(x,dim0=1,dim1=2)\n",
    "        return x\n",
    "\n",
    "# class Encoder(nn.Module):\n",
    "#     def __init__(self, p_dropout=0.5):\n",
    "#         super(Encoder, self).__init__()\n",
    "#         self.filter_number = ENV_CNN_FILTERS\n",
    "#         self.kernel_number = ENV_CNN_KERNELS  # tedad size haye filter : 2,3,5 = 3\n",
    "#         self.embedding_size = ENV_EMBEDDING_SIZE\n",
    "#         # self.activation = nn.ReLU()\n",
    "#         # self.activation = RMSNorm(ENV_CNN_FILTERS)\n",
    "#         self.activation = nn.LeakyReLU()\n",
    "#         self.softmax = nn.Softmax(dim=1)\n",
    "#         # self.linear1 = nn.Linear(self.embedding_size, self.filter_number)\n",
    "#         self.conv1 = nn.Conv1d(in_channels=self.embedding_size, out_channels=self.filter_number, kernel_size=(1,),\n",
    "#                                padding=\"same\", padding_mode=\"zeros\")\n",
    "#         self.conv2 = nn.Conv1d(in_channels=self.embedding_size, out_channels=self.filter_number, kernel_size=(3,),\n",
    "#                                padding=\"same\", padding_mode=\"zeros\")\n",
    "#         self.conv3 = nn.Conv1d(in_channels=self.embedding_size, out_channels=self.filter_number, kernel_size=(3,),\n",
    "#                                padding=\"same\", padding_mode=\"zeros\",dilation=3)\n",
    "\n",
    "#         self.conv4 = nn.Conv1d(in_channels=self.embedding_size, out_channels=self.filter_number, kernel_size=(5,),\n",
    "#                                padding=\"same\", padding_mode=\"zeros\")\n",
    "\n",
    "#         self.dropout = nn.Dropout(p_dropout)\n",
    "#         self.layer_norm = nn.LayerNorm(ENV_EMBEDDING_SIZE)\n",
    "#         self.senet = SENET(ENV_EMBEDDING_SIZE)\n",
    "#         self.V = nn.Parameter(torch.randn(self.embedding_size))\n",
    "\n",
    "#         self.tanh_linear = nn.Linear(self.embedding_size, self.embedding_size)\n",
    "#         self.tanh_activation = nn.Tanh()\n",
    "#         self.softmax_activation =nn.Sigmoid()               # nn.Softmax(-1)\n",
    "\n",
    "#     def forward(self, bert_last_hidden,bert_pooler_output=None,isid=False):\n",
    "#         trans_embedded = torch.transpose(bert_last_hidden, dim0=1, dim1=2)\n",
    "#         # linear1 = self.activation(self.linear1(bert_last_hidden))\n",
    "#         convolve1 = self.activation(torch.transpose(self.conv1(trans_embedded), dim0=1, dim1=2))\n",
    "#         convolve2 = self.activation(torch.transpose(self.conv2(trans_embedded), dim0=1, dim1=2))\n",
    "#         convolve3 = self.activation(torch.transpose(self.conv3(trans_embedded), dim0=1, dim1=2))\n",
    "#         convolve4 = self.activation(torch.transpose(self.conv4(trans_embedded), dim0=1, dim1=2))\n",
    "\n",
    "#         output = torch.cat((convolve1, convolve2,convolve3,convolve4), dim=2)\n",
    "#         output = torch.sum(self.V * self.tanh_activation(self.tanh_linear(output)),dim = -1)\n",
    "#         # if isid:\n",
    "#         #     output += bert_pooler_output\n",
    "#         output = self.softmax_activation(output)\n",
    "#         output = torch.unsqueeze(output,-1)\n",
    "#         output = output * bert_last_hidden\n",
    "#         # output = self.layer_norm(self.dropout(output)) + bert_last_hidden\n",
    "#         # output = self.senet(output)\n",
    "#         return output\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, p_dropout=0.5,patch_size=32):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.filter_number = ENV_CNN_FILTERS\n",
    "        self.kernel_number = ENV_CNN_KERNELS  # tedad size haye filter : 2,3,5 = 3\n",
    "        self.embedding_size = ENV_EMBEDDING_SIZE\n",
    "        self.activation = nn.LeakyReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.conv1 = nn.Conv2d(in_channels=LENGTH, out_channels=LENGTH, kernel_size=(1,1),\n",
    "                               padding=\"same\", padding_mode=\"zeros\")\n",
    "        self.conv2 = nn.Conv2d(in_channels=LENGTH, out_channels=LENGTH, kernel_size=(3,3),\n",
    "                               padding=\"same\", padding_mode=\"zeros\")\n",
    "        self.conv3 = nn.Conv2d(in_channels=LENGTH, out_channels=LENGTH, kernel_size=(3,3),\n",
    "                               padding=\"same\", padding_mode=\"zeros\",dilation=3)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(in_channels=LENGTH, out_channels=LENGTH, kernel_size=(5,5),\n",
    "                               padding=\"same\", padding_mode=\"zeros\")\n",
    "\n",
    "        self.dropout = nn.Dropout(p_dropout)\n",
    "        self.layer_norm = nn.LayerNorm(ENV_EMBEDDING_SIZE)\n",
    "        self.senet = SENET(ENV_EMBEDDING_SIZE)\n",
    "        self.V = nn.Parameter(torch.randn(self.embedding_size))\n",
    "\n",
    "        self.tanh_linear = nn.Linear(self.embedding_size, self.embedding_size)\n",
    "        self.tanh_activation = nn.Tanh()\n",
    "        self.softmax_activation =nn.Sigmoid()               # nn.Softmax(-1)\n",
    "\n",
    "    def forward(self, bert_last_hidden,bert_pooler_output=None,isid=False):\n",
    "        batch,seq_len,hidden_dim = bert_last_hidden.shape\n",
    "        reshaped_embedded = torch.reshape(bert_last_hidden,shape=(batch,seq_len,self.patch_size,-1))\n",
    "        convolve1 = torch.reshape(self.conv1(reshaped_embedded),shape=(batch,seq_len,hidden_dim))\n",
    "        convolve2 = torch.reshape(self.conv2(reshaped_embedded),shape=(batch,seq_len,hidden_dim))\n",
    "        convolve3 = torch.reshape(self.conv3(reshaped_embedded),shape=(batch,seq_len,hidden_dim))\n",
    "        convolve4 = torch.reshape(self.conv4(reshaped_embedded),shape=(batch,seq_len,hidden_dim))\n",
    "\n",
    "        output = convolve1 + convolve2 + convolve3 + convolve4\n",
    "        output = torch.sum(self.V * self.tanh_activation(self.tanh_linear(output)),dim = -1)\n",
    "\n",
    "        output = self.softmax_activation(output)\n",
    "        output = torch.unsqueeze(output,-1)\n",
    "        output = output * bert_last_hidden\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from torch import nn\n",
    "from functools import partial\n",
    "\n",
    "pair = lambda x: x if isinstance(x, tuple) else (x, x)\n",
    "\n",
    "class PreNormResidual(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fn(self.norm(x)) + x\n",
    "\n",
    "def FeedForward(dim, expansion_factor = 4, dropout = 0., dense = nn.Linear):\n",
    "    inner_dim = int(dim * expansion_factor)\n",
    "    return nn.Sequential(\n",
    "        dense(dim, inner_dim),\n",
    "        nn.GELU(),\n",
    "        nn.Dropout(dropout),\n",
    "        dense(inner_dim, dim),\n",
    "        nn.Dropout(dropout)\n",
    "    )\n",
    "\n",
    "class IDClassifier(nn.Module):\n",
    "    def __init__(self, depth, dim, num_classes, expansion_factor = 4, expansion_factor_token = 0.5, dropout = 0.):\n",
    "        super(IDClassifier,self).__init__()\n",
    "        self.num_tokens = LENGTH\n",
    "        self.chan_first, self.chan_last = partial(nn.Conv1d, kernel_size = 1), nn.Linear\n",
    "        \n",
    "        self.class_mlp = nn.Sequential(\n",
    "            *[nn.Sequential(\n",
    "                PreNormResidual(dim, FeedForward(self.num_tokens, expansion_factor, dropout, self.chan_first)),\n",
    "                PreNormResidual(dim, FeedForward(dim, expansion_factor_token, dropout, self.chan_last))\n",
    "            ) for _ in range(depth)],\n",
    "            nn.LayerNorm(dim)  \n",
    "        )\n",
    "        self.activation2 = nn.ReLU()\n",
    "        self.classifier = nn.Linear(dim, num_classes)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.squeeze_linear = nn.Linear(LENGTH, LENGTH //4)\n",
    "        self.activation2 = nn.ReLU()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.class_mlp(x)\n",
    "        x=torch.transpose(x,dim0=1,dim1=2)\n",
    "        x = self.activation2(self.dropout1(self.squeeze_linear(x)))\n",
    "        x = torch.transpose(x,dim0=1,dim1=2).mean(1)\n",
    "        x = self.classifier(self.dropout1(x))\n",
    "\n",
    "        x = F.log_softmax(x,dim=1)      \n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class id_subnet(nn.Module):\n",
    "#     def __init__(self,hidden_size):\n",
    "#         self.hidden_size = hidden_size\n",
    "\n",
    "#         self.id_subnet_linear1 = nn.Linear(hidden_size,hidden_size) \n",
    "#         self.id_subnet_relu = nn.ReLU()\n",
    "#         self.id_subnet_tanh = nn.Tanh()\n",
    "#         self.id_subnet_v1 = nn.Parameter(torch.randn(hidden_size))\n",
    "\n",
    "#     def forward(self,id_attn, sf_attn,bert_output):\n",
    "#         sf = self.id_subnet_relu(self.id_subnet_linear1(sf_attn))\n",
    "#         x = sf + id_attn\n",
    "#         x = torch.sum(self.id_subnet_v1 * self.id_subnet_tanh(x), -1)\n",
    "#         x *= id_attn\n",
    "\n",
    "#         x = torch.concat((x, bert_output), 1)\n",
    "\n",
    "#         return x\n",
    "\n",
    "# class sf_subnet(nn.Module):\n",
    "#     def __init__(self,hidden_size):\n",
    "#         self.hidden_size = hidden_size\n",
    "\n",
    "#         self.sf_subnet_linear1 = nn.Linear(hidden_size,hidden_size) \n",
    "#         self.sf_subnet_relu = nn.ReLU()\n",
    "#         self.sf_subnet_tanh = nn.Tanh()\n",
    "#         self.sf_subnet_v1 = nn.Parameter(torch.randn(hidden_size))\n",
    "\n",
    "#     def forward(self,id_attn, sf_attn,bert_output):\n",
    "        \n",
    "\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Middle\n",
    "class Middle(nn.Module):\n",
    "    def __init__(self ,p_dropout=0.5):\n",
    "        super().__init__()\n",
    "        # self.activation = nn.ReLU()\n",
    "        # self.p_dropout = p_dropout\n",
    "        # self.softmax = nn.Softmax(dim=1)\n",
    "        #Transformer\n",
    "        nlayers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "        self.pos_encoder = PositionalEncoding(ENV_HIDDEN_SIZE, dropout=0.1)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(ENV_HIDDEN_SIZE, nhead=2,batch_first=True, dim_feedforward=2048 ,activation=\"relu\", dropout=0.1)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers,enable_nested_tensor=False)\n",
    "        self.middle_linear = nn.Linear(ENV_EMBEDDING_SIZE,ENV_EMBEDDING_SIZE)\n",
    "        self.middle_dropout = nn.Dropout(p_dropout)\n",
    "        self.middle_tanh = nn.Tanh()\n",
    "        \n",
    "\n",
    "    def forward(self, fromencoder,input_masking,training=True):\n",
    "        src = fromencoder * math.sqrt(ENV_HIDDEN_SIZE)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = (self.transformer_encoder(src,src_key_padding_mask=input_masking)) # outputs probably\n",
    "        output = self.middle_tanh(self.middle_dropout(self.middle_linear(output)))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start of the decoder\n",
    "# from torchcrf import CRF\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self,slot_size,intent_size,dropout_p=0.7,ratio_squeeze= 4):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.slot_size = slot_size\n",
    "        self.intent_size = intent_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.softmax= nn.Softmax(dim=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        # Define the layers\n",
    "        self.embedding = nn.Embedding(self.slot_size, ENV_HIDDEN_SIZE)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(self.dropout_p)\n",
    "        self.dropout2 = nn.Dropout(self.dropout_p)\n",
    "        self.dropout3 = nn.Dropout(self.dropout_p)\n",
    "        self.slot_trans = nn.Linear(ENV_HIDDEN_SIZE, self.slot_size)\n",
    "        self.intent_out = nn.Linear(ENV_HIDDEN_SIZE,self.intent_size)\n",
    "        self.intent_out_cls = nn.Linear(ENV_EMBEDDING_SIZE,self.intent_size) # dim of bert\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=ENV_HIDDEN_SIZE, nhead=4,batch_first=True,dim_feedforward=512 ,activation=\"relu\")\n",
    "        self.transformer_decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=4)\n",
    "        self.transformer_mask = generate_square_subsequent_mask(LENGTH).cuda()\n",
    "        self.transformer_diagonal_mask = generate_square_diagonal_mask(LENGTH).cuda()\n",
    "        self.pos_encoder = PositionalEncoding(ENV_HIDDEN_SIZE, dropout=0.1)\n",
    "        self.self_attention = nn.MultiheadAttention(embed_dim=ENV_HIDDEN_SIZE\n",
    "                                                    ,num_heads=12,dropout=0.1\n",
    "                                                    ,batch_first=True)\n",
    "        self.layer_norm = nn.LayerNorm(ENV_HIDDEN_SIZE)\n",
    "\n",
    "        self.dropout4 = nn.Dropout(self.dropout_p)\n",
    "        self.squeeze_linear = nn.Linear(LENGTH, LENGTH //ratio_squeeze)\n",
    "        self.activation2 = nn.Tanh()\n",
    "        # self.activation2 = RMSNorm(LENGTH // ratio_squeeze)\n",
    "        self.IDclassifier = IDClassifier(DEPTH,ENV_HIDDEN_SIZE,intent_size,dropout=0.5)\n",
    "\n",
    "        self.squeeze_linear_sf = nn.Linear(ENV_HIDDEN_SIZE, self.slot_size)\n",
    "        self.sf_dropout = nn.Dropout(self.dropout_p)\n",
    "        self.sf_relu = nn.Tanh()\n",
    "        self.sf_classifier = nn.Linear(ENV_HIDDEN_SIZE // 4,self.slot_size)\n",
    "        self.sf_drop2 = nn.Dropout(self.dropout_p)\n",
    "\n",
    "        self.id_attn = Encoder()\n",
    "        self.sf_attn = Encoder()\n",
    "        self.middle = Middle()\n",
    "\n",
    "        self.cross_task_linear = nn.Linear(ENV_HIDDEN_SIZE,ENV_HIDDEN_SIZE)\n",
    "        self.cross_task_dropout = nn.Dropout(self.dropout_p)\n",
    "        self.cross_task_tanh = nn.Tanh()\n",
    "\n",
    "        # self.bert_pooler_linear = nn.Linear(LENGTH, LENGTH //ratio_squeeze)\n",
    "        # self.bert_pooler_dropout = nn.Dropout(self.dropout_p)\n",
    "        # self.bert_pooler_relu = nn.ReLU()\n",
    "\n",
    "\n",
    "        # self.crf_layer = CRF(slot_size)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input,bert_outputs,encoder_maskings,bert_subtoken_maskings=None,infer=False,bert_pooler_outputs=None):\n",
    "        id_attn = self.id_attn(bert_outputs, bert_pooler_outputs,True)\n",
    "        sf_attn = self.sf_attn(bert_outputs)\n",
    "\n",
    "        # cross_attn = id_attn + sf_attn\n",
    "        # cross_attn = torch.concat([id_attn,sf_attn], dim=1)\n",
    "        # cross_attn = id_attn + sf_attn + bert_outputs\n",
    "\n",
    "        # cross_attn = self.middle(cross_attn,encoder_maskings==0)\n",
    "        cross_attn_id,_ = self.self_attention(id_attn,sf_attn,sf_attn,key_padding_mask=encoder_maskings)\n",
    "        cross_attn_id = self.cross_task_tanh(self.cross_task_dropout(self.cross_task_linear(cross_attn_id)))\n",
    "        cross_attn_sf,_ = self.self_attention(sf_attn,id_attn,id_attn,key_padding_mask=encoder_maskings)\n",
    "        cross_attn_sf = self.cross_task_tanh(self.cross_task_dropout(self.cross_task_linear(cross_attn_sf)))\n",
    "\n",
    "\n",
    "        cross_id = cross_attn_id + id_attn\n",
    "        cross_sf = sf_attn + cross_attn_sf\n",
    "        # cross_id = id_attn + cross_attn\n",
    "        # cross_sf = sf_attn + cross_attn\n",
    "\n",
    "        # sum_mask = (~encoder_maskings).sum(1).unsqueeze(1)\n",
    "        # sum_encoder = ((((cross_id)))*((~encoder_maskings).unsqueeze(2))).sum(1)\n",
    "        # intent_scores = self.intent_out(self.dropout1(sum_encoder/sum_mask)) # B,D\n",
    "\n",
    "        cross_id=torch.transpose(cross_id,dim0=1,dim1=2)\n",
    "        cross_id = self.activation2(self.dropout4(self.squeeze_linear(cross_id)))\n",
    "        cross_id = torch.transpose(cross_id,dim0=1,dim1=2).mean(1)\n",
    "        \n",
    "        intent_scores = self.intent_out(self.dropout1(cross_id))\n",
    "        intent_scores = F.log_softmax(intent_scores,dim=1)   \n",
    "\n",
    "\n",
    "        # slot_scores = self.sf_dropout(self.squeeze_linear_sf(cross_sf))\n",
    "        # slot_scores = F.log_softmax(slot_scores,dim=-1) \n",
    "        # cross_sf = self.crf_layer(cross_sf)\n",
    "        # intent_score = self.IDclassifier(encoder_outputs)                     \n",
    "\n",
    "        batch_size = cross_sf.shape[0]\n",
    "        length = cross_sf.size(1) #for every token in batches\n",
    "        embedded = self.embedding(input)\n",
    "        newtensor = torch.cuda.FloatTensor(batch_size, length,ENV_HIDDEN_SIZE).fill_(0.) # size of newtensor same as original\n",
    "        for i in range(batch_size): # per batch\n",
    "            newtensor_index=0\n",
    "            for j in range(length): # for each token\n",
    "                if bert_subtoken_maskings[i][j].item()==1:\n",
    "                    newtensor[i][newtensor_index] = cross_sf[i][j]\n",
    "                    newtensor_index+=1\n",
    "\n",
    "        if infer==False:\n",
    "            embedded=embedded*math.sqrt(ENV_HIDDEN_SIZE)\n",
    "            embedded = self.pos_encoder(embedded)\n",
    "            zol = self.transformer_decoder(tgt=embedded,memory=newtensor\n",
    "                                           ,memory_mask=self.transformer_diagonal_mask\n",
    "                                           ,tgt_mask=self.transformer_mask)\n",
    "\n",
    "            scores = self.slot_trans(self.dropout3(zol))    # B, S, 124\n",
    "            slot_scores = F.log_softmax(scores,dim=2)\n",
    "        else:\n",
    "            bos = Variable(torch.LongTensor([[tag2index['<BOS>']]*batch_size])).cuda().transpose(1,0)\n",
    "            bos = self.embedding(bos)\n",
    "            tokens=bos\n",
    "            for i in range(length):\n",
    "                temp_embedded=tokens*math.sqrt(ENV_HIDDEN_SIZE)\n",
    "                temp_embedded = self.pos_encoder(temp_embedded)\n",
    "                zol = self.transformer_decoder(tgt=temp_embedded,\n",
    "                                               memory=newtensor,\n",
    "                                               tgt_mask=self.transformer_mask[:i+1,:i+1],\n",
    "                                               memory_mask=self.transformer_diagonal_mask[:i+1,:]\n",
    "                                               )\n",
    "                scores = self.slot_trans(self.dropout3(zol))\n",
    "                softmaxed = F.log_softmax(scores,dim=2)\n",
    "                #the last token is apended to vectors\n",
    "                _,input = torch.max(softmaxed,2)\n",
    "                newtok = self.embedding(input)\n",
    "                tokens=torch.cat((bos,newtok),dim=1)\n",
    "            slot_scores = softmaxed\n",
    "\n",
    "\n",
    "        return slot_scores.view(input.size(0)*length,-1), intent_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# #start of the decoder\n",
    "# class Decoder(nn.Module):\n",
    "\n",
    "#     def __init__(self,slot_size,intent_size,dropout_p=0.7,ratio_squeeze= 4):\n",
    "#         super(Decoder, self).__init__()\n",
    "#         self.slot_size = slot_size\n",
    "#         self.intent_size = intent_size\n",
    "#         self.dropout_p = dropout_p\n",
    "#         self.softmax= nn.Softmax(dim=1)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "#         # Define the layers\n",
    "#         self.embedding = nn.Embedding(self.slot_size, ENV_HIDDEN_SIZE)\n",
    "#         self.activation = nn.ReLU()\n",
    "#         self.dropout1 = nn.Dropout(self.dropout_p)\n",
    "#         self.dropout2 = nn.Dropout(self.dropout_p)\n",
    "#         self.dropout3 = nn.Dropout(self.dropout_p)\n",
    "#         self.slot_trans = nn.Linear(ENV_HIDDEN_SIZE, self.slot_size)\n",
    "#         self.intent_out = nn.Linear(ENV_HIDDEN_SIZE,self.intent_size)\n",
    "#         self.intent_out_cls = nn.Linear(ENV_EMBEDDING_SIZE,self.intent_size) # dim of bert\n",
    "#         self.decoder_layer = nn.TransformerDecoderLayer(d_model=ENV_HIDDEN_SIZE, nhead=4,batch_first=True,dim_feedforward=512 ,activation=\"relu\")\n",
    "#         self.transformer_decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=4)\n",
    "#         self.transformer_mask = generate_square_subsequent_mask(LENGTH).cuda()\n",
    "#         self.transformer_diagonal_mask = generate_square_diagonal_mask(LENGTH).cuda()\n",
    "#         self.pos_encoder = PositionalEncoding(ENV_HIDDEN_SIZE, dropout=0.1)\n",
    "#         self.self_attention = nn.MultiheadAttention(embed_dim=ENV_HIDDEN_SIZE\n",
    "#                                                     ,num_heads=8,dropout=0.1\n",
    "#                                                     ,batch_first=True)\n",
    "#         self.layer_norm = nn.LayerNorm(ENV_HIDDEN_SIZE)\n",
    "\n",
    "#         self.dropout4 = nn.Dropout(self.dropout_p)\n",
    "#         self.squeeze_linear = nn.Linear(LENGTH, LENGTH //ratio_squeeze)\n",
    "#         self.activation2 = nn.ReLU()\n",
    "#         # self.activation2 = RMSNorm(LENGTH // ratio_squeeze)\n",
    "#         self.IDclassifier = IDClassifier(DEPTH,ENV_HIDDEN_SIZE,intent_size,dropout=0.5)\n",
    "\n",
    "#         self.squeeze_linear_sf = nn.Linear(ENV_HIDDEN_SIZE, ENV_HIDDEN_SIZE // 4)\n",
    "#         self.sf_dropout = nn.Dropout(self.dropout_p)\n",
    "#         self.sf_relu = nn.ReLU()\n",
    "#         self.sf_classifier = nn.Linear(ENV_HIDDEN_SIZE // 4,self.slot_size)\n",
    "#         self.sf_drop2 = nn.Dropout(self.dropout_p)\n",
    "\n",
    "\n",
    "\n",
    "#     def forward(self, input,encoder_outputs,encoder_maskings,bert_subtoken_maskings=None,infer=False):\n",
    "#         # encoder outputs: BATCH,LENGTH,Dims (16,60,1024)\n",
    "#         batch_size = encoder_outputs.shape[0]\n",
    "#         length = encoder_outputs.size(1) #for every token in batches\n",
    "#         embedded = self.embedding(input)\n",
    "\n",
    "\n",
    "#         encoder_outputs2=torch.transpose(encoder_outputs,dim0=1,dim1=2)\n",
    "#         encoder_outputs2 = self.activation2(self.dropout4(self.squeeze_linear(encoder_outputs2)))\n",
    "#         encoder_outputs2 = torch.transpose(encoder_outputs2,dim0=1,dim1=2).mean(1)\n",
    "#         # encoder_outputs2 = torch.argmax(encoder_outputs,dim=1)\n",
    "#         intent_score = self.intent_out(self.dropout1(encoder_outputs2))\n",
    "#         intent_score = F.log_softmax(intent_score,dim=1)   \n",
    "#         # intent_score = self.IDclassifier(encoder_outputs)                     \n",
    "\n",
    "#         # slot_scores = self.sf_relu(self.sf_dropout(self.squeeze_linear_sf(cnn_wfs)))\n",
    "#         # slot_scores = self.sf_classifier(self.sf_drop2(slot_scores))\n",
    "#         # slot_scores = F.log_softmax(slot_scores,dim=-1)\n",
    "#         newtensor = torch.cuda.FloatTensor(batch_size, length,ENV_HIDDEN_SIZE).fill_(0.) # size of newtensor same as original\n",
    "#         for i in range(batch_size): # per batch\n",
    "#             newtensor_index=0\n",
    "#             for j in range(length): # for each token\n",
    "#                 if bert_subtoken_maskings[i][j].item()==1:\n",
    "#                     newtensor[i][newtensor_index] = encoder_outputs[i][j]\n",
    "#                     newtensor_index+=1\n",
    "\n",
    "#         if infer==False:\n",
    "#             embedded=embedded*math.sqrt(ENV_HIDDEN_SIZE)\n",
    "#             embedded = self.pos_encoder(embedded)\n",
    "#             zol = self.transformer_decoder(tgt=embedded,memory=newtensor\n",
    "#                                            ,memory_mask=self.transformer_diagonal_mask\n",
    "#                                            ,tgt_mask=self.transformer_mask)\n",
    "\n",
    "#             scores = self.slot_trans(self.dropout3(zol))    # B, S, 124\n",
    "#             slot_scores = F.log_softmax(scores,dim=2)\n",
    "#         else:\n",
    "#             bos = Variable(torch.LongTensor([[tag2index['<BOS>']]*batch_size])).cuda().transpose(1,0)\n",
    "#             bos = self.embedding(bos)\n",
    "#             tokens=bos\n",
    "#             for i in range(length):\n",
    "#                 temp_embedded=tokens*math.sqrt(ENV_HIDDEN_SIZE)\n",
    "#                 temp_embedded = self.pos_encoder(temp_embedded)\n",
    "#                 zol = self.transformer_decoder(tgt=temp_embedded,\n",
    "#                                                memory=newtensor,\n",
    "#                                                tgt_mask=self.transformer_mask[:i+1,:i+1],\n",
    "#                                                memory_mask=self.transformer_diagonal_mask[:i+1,:]\n",
    "#                                                )\n",
    "#                 scores = self.slot_trans(self.dropout3(zol))\n",
    "#                 softmaxed = F.log_softmax(scores,dim=2)\n",
    "#                 #the last token is apended to vectors\n",
    "#                 _,input = torch.max(softmaxed,2)\n",
    "#                 newtok = self.embedding(input)\n",
    "#                 tokens=torch.cat((bos,newtok),dim=1)\n",
    "#             slot_scores = softmaxed\n",
    "\n",
    "#         return slot_scores.view(slot_scores.size(0)*length,-1), intent_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.bert_layer = BertLayer()\n",
    "        self.decoder = Decoder(len(tag2index),len(intent2index))\n",
    "\n",
    "    def forward(self,bert_tokens,bert_mask,bert_toktype,subtoken_mask,tag_target=None,infer=False):\n",
    "        batch_size=bert_tokens.size(0)\n",
    "        bert_hidden,bert_pooler_outputs = self.bert_layer(bert_info=(bert_tokens,bert_mask,bert_toktype))\n",
    "\n",
    "\n",
    "        start_decode = Variable(torch.LongTensor([[tag2index['<BOS>']]*batch_size])).cuda().transpose(1,0)\n",
    "        \n",
    "        if not infer:\n",
    "            start_decode = torch.cat((start_decode,tag_target[:,:-1]),dim=1)\n",
    "\n",
    "        # tag score shape 960,124,  tag target 16,60\n",
    "        # intent scoer shape 16,22,  intent target shape 16\n",
    "        # tag_score, intent_score = self.decoder(start_decode,output,bert_mask==0,bert_subtoken_maskings=subtoken_mask,infer=infer)\n",
    "        tag_score, intent_score = self.decoder(start_decode,bert_hidden,bert_mask==0,bert_subtoken_maskings=subtoken_mask,infer=infer,bert_pooler_outputs=bert_pooler_outputs)\n",
    "\n",
    "        return tag_score, intent_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "from crf import CRF\n",
    "loss_function_1 = nn.CrossEntropyLoss(ignore_index=0)\n",
    "crf_log_likelihood = CRF(num_tags=len(tag2index),batch_first=True).cuda()\n",
    "loss_function_2 = nn.CrossEntropyLoss()\n",
    "loss_function_3 = nn.L1Loss()\n",
    "\n",
    "model = Model()\n",
    "if USE_CUDA:\n",
    "    model.cuda()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0001,weight_decay=0.8)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, train_data, postfix_dict):\n",
    "    losses=[]\n",
    "    loss_1es = []\n",
    "    loss_2es = []\n",
    "    id_precision=[]\n",
    "    sf_f1=[]\n",
    "    model.train()\n",
    "    for i,(x,tag_target,intent_target,bert_tokens,bert_mask,bert_toktype,subtoken_mask,x_mask) in enumerate(train_data):\n",
    "        batch_size=tag_target.size(0)\n",
    "        tag_score, intent_score = model(bert_tokens,bert_mask,bert_toktype,subtoken_mask,tag_target)\n",
    "\n",
    "        perd_intent = torch.argmax(intent_score,dim=1)\n",
    "        # loss_1 = loss_function_1(tag_score,tag_target.view(-1))\n",
    "        # tag_score = torch.transpose(tag_score,dim0=0,dim1=1)\n",
    "        # tag_target = torch.transpose(tag_target,dim0=0,dim1=1)\n",
    "        tag_scores = tag_score.view(batch_size, LENGTH, -1)\n",
    "        loss_1 = - crf_log_likelihood.log_likelihood(tag_scores, tag_target, reduction=\"mean\") + loss_function_1(tag_score,tag_target.view(-1))\n",
    "        # loss_1 = loss_function_1(tag_score,tag_target.view(-1))\n",
    "        loss_2 = loss_function_2(intent_score,intent_target)\n",
    "        # loss_3 = loss_function_3(perd_intent.float(),intent_target)\n",
    "        loss = loss_1 + loss_2\n",
    "        # loss = loss_2 + loss_3\n",
    "\n",
    "        loss_1es.append(loss_1.data.cpu().numpy() if USE_CUDA else loss_1.data.numpy()[0])\n",
    "        loss_2es.append(loss_2.data.cpu().numpy() if USE_CUDA else loss_2.data.numpy()[0])\n",
    "        losses.append(loss.data.cpu().numpy() if USE_CUDA else loss.data.numpy()[0])\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "\n",
    "        optimizer.step()\n",
    "        id_precision.append(accuracy_score(intent_target.detach().cpu(),torch.argmax(intent_score,dim=1).detach().cpu()))\n",
    "\n",
    "        # tag_score = crf_log_likelihood.decode(tag_score)\n",
    "        # pred_list,target_list=mask_important_tags(tag_score,tag_target,x_mask)\n",
    "        pred_list,target_list=mask_important_tags(torch.argmax(tag_score,dim=1).view(batch_size,LENGTH),tag_target,x_mask)\n",
    "        sf_f1.append(f1_score(pred_list,target_list,average=\"micro\",zero_division=0))\n",
    "\n",
    "        postfix_dict[\"T_L\"] = round(float(np.mean(losses)),4)\n",
    "        postfix_dict[\"SF_L\"] = round(float(np.mean(loss_1es)),4)\n",
    "        postfix_dict[\"ID_L\"] = round(float(np.mean(loss_2es)),4)\n",
    "        postfix_dict[\"SF_F1\"] = round(float(np.mean(sf_f1)),5) * 100\n",
    "        postfix_dict[\"ID_P\"] = round(float(np.mean(id_precision)),5) * 100\n",
    "\n",
    "    return postfix_dict,model\n",
    "\n",
    "\n",
    "def eval_loop(model, test_data):\n",
    "    postfix_dict = {}\n",
    "    losses=[]\n",
    "    loss_1es = []\n",
    "    loss_2es = []\n",
    "    id_precision=[]\n",
    "    sf_f1=[]\n",
    "    model.eval()\n",
    "    with torch.no_grad(): # to turn off gradients computation\n",
    "        for i,(x,tag_target,intent_target,bert_tokens,bert_mask,bert_toktype,subtoken_mask,x_mask) in enumerate(test_data):\n",
    "            batch_size=tag_target.size(0)\n",
    "            model.zero_grad()\n",
    "            \n",
    "            tag_score, intent_score = model(bert_tokens,bert_mask,bert_toktype,subtoken_mask,infer=True)\n",
    "            # tag_score = torch.transpose(tag_score,dim0=0,dim1=1)\n",
    "            # tag_target = torch.transpose(tag_target,dim0=0,dim1=1)\n",
    "            # loss_1 = torch.mean(-1 * crf_log_likelihood(tag_score,tag_target))\n",
    "            tag_scores = tag_score.view(batch_size, LENGTH, -1)\n",
    "            loss_1 = - crf_log_likelihood.log_likelihood(tag_scores, tag_target, reduction=\"mean\")\n",
    "            # loss_1 = loss_function_1(tag_score,tag_target.view(-1))\n",
    "            loss_2 = loss_function_2(intent_score,intent_target)\n",
    "            loss = loss_1 + loss_2\n",
    "            loss_1es.append(loss_1.data.cpu().numpy() if USE_CUDA else loss_1.data.numpy()[0])\n",
    "            loss_2es.append(loss_2.data.cpu().numpy() if USE_CUDA else loss_2.data.numpy()[0])\n",
    "            losses.append(loss.data.cpu().numpy() if USE_CUDA else loss.data.numpy()[0])\n",
    "            id_precision.append(accuracy_score(intent_target.detach().cpu(),torch.argmax(intent_score,dim=1).detach().cpu()))\n",
    "\n",
    "            # tag_score = crf_log_likelihood.decode(tag_score)\n",
    "            # pred_list,target_list=mask_important_tags(tag_score,tag_target,x_mask)\n",
    "            pred_list,target_list=mask_important_tags(torch.argmax(tag_score,dim=1).view(batch_size,LENGTH),tag_target,x_mask)\n",
    "            sf_f1.append(f1_score(pred_list,target_list,average=\"micro\",zero_division=0))\n",
    "\n",
    "        postfix_dict[\"Te_L\"] = round(float(np.mean(losses)),4)\n",
    "        postfix_dict[\"Te_SF_L\"] = round(float(np.mean(loss_1es)),4)\n",
    "        postfix_dict[\"Te_ID_L\"] = round(float(np.mean(loss_2es)),4)\n",
    "        postfix_dict[\"Te_SF_F1\"] = round(float(np.mean(sf_f1)),5) * 100\n",
    "        postfix_dict[\"Te_ID_P\"] = round(float(np.mean(id_precision)),5) * 100\n",
    "\n",
    "    return postfix_dict, round(float(np.mean(sf_f1)),5), round(float(np.mean(id_precision)),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_577617/2887580963.py:101: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)\n",
      "  newtensor = torch.cuda.FloatTensor(batch_size, length,ENV_HIDDEN_SIZE).fill_(0.) # size of newtensor same as original\n",
      "Epoch: 100%|██████████| 50/50 [22:21<00:00, 26.82s/it, T_L=0.0317, SF_L=0.0282, ID_L=0.0035, SF_F1=100, ID_P=99.9, Te_L=3.09, Te_SF_L=2.48, Te_ID_L=0.611, Te_SF_F1=98.3, Te_ID_P=97.8, Best_ID_P=97.9, Best_SF_F1=98.1] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max single ID PR: 0.97879\n",
      "max single SF F1: 0.98292\n",
      "max mutual PR: 0.97879   SF:0.9809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_id_prec=0.\n",
    "max_sf_f1=0.\n",
    "max_id_prec_both=0.\n",
    "max_sf_f1_both=0.\n",
    "\n",
    "with tqdm(total=STEP_SIZE,desc=\"Epoch\") as epoiter:\n",
    "    for step in range(STEP_SIZE):\n",
    "        postfix_dict = {}\n",
    "\n",
    "        postfix_dict,model = train_loop(model, train_data, postfix_dict)\n",
    "        eval_dict,sf_f1,id_precision = eval_loop(model, test_data)\n",
    "\n",
    "        # Save Best\n",
    "        max_sf_f1 = max_sf_f1 if sf_f1<=max_sf_f1 else sf_f1\n",
    "        max_id_prec = max_id_prec if id_precision<=max_id_prec else id_precision\n",
    "        if max_sf_f1_both<=sf_f1 and max_id_prec_both<=id_precision:\n",
    "            max_sf_f1_both=sf_f1\n",
    "            max_id_prec_both=id_precision\n",
    "            torch.save(model,f\"models/model.pkl\")\n",
    "        \n",
    "        postfix_dict.update(eval_dict)\n",
    "        postfix_dict[\"Best_ID_P\"] = round(max_id_prec_both * 100,3)\n",
    "        postfix_dict[\"Best_SF_F1\"] = round(max_sf_f1_both * 100,3)\n",
    "        \n",
    "        scheduler.step()\n",
    "        epoiter.set_postfix(postfix_dict)\n",
    "        epoiter.update(1)\n",
    "    \n",
    "    print(f\"max single ID PR: {max_id_prec}\")\n",
    "    print(f\"max single SF F1: {max_sf_f1}\")\n",
    "    print(f\"max mutual PR: {max_id_prec_both}   SF:{max_sf_f1_both}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# max_id_prec=0.\n",
    "# max_sf_f1=0.\n",
    "# max_id_prec_both=0.\n",
    "# max_sf_f1_both=0.\n",
    "\n",
    "\n",
    "# with tqdm(total=STEP_SIZE,desc=\"Epoch\") as epoiter:\n",
    "#     for step in range(STEP_SIZE):\n",
    "#         postfix_dict = {}\n",
    "#         losses=[]\n",
    "#         loss_1es = []\n",
    "#         loss_2es = []\n",
    "#         id_precision=[]\n",
    "#         sf_f1=[]\n",
    "\n",
    "#         ### TRAIN\n",
    "#         encoder.train() # set to train mode\n",
    "#         middle.train()\n",
    "#         decoder.train()\n",
    "#         bert_layer.train()\n",
    "#         for i,(x,tag_target,intent_target,bert_tokens,bert_mask,bert_toktype,subtoken_mask,x_mask) in enumerate(train_data):\n",
    "#             batch_size=tag_target.size(0)\n",
    "#             bert_layer.zero_grad()\n",
    "#             encoder.zero_grad()\n",
    "#             middle.zero_grad()\n",
    "#             decoder.zero_grad()\n",
    "#             # bert_hidden shape 16,60,768\n",
    "#             # bert pooler shape 16, 768\n",
    "#             bert_hidden,bert_pooler = bert_layer(bert_info=(bert_tokens,bert_mask,bert_toktype))\n",
    "\n",
    "#             # encoder output shape 16,60,512\n",
    "#             encoder_output = encoder(bert_last_hidden=bert_hidden)\n",
    "\n",
    "#             # output shape same as encoder output\n",
    "#             output = middle(encoder_output,bert_mask==0,training=True)\n",
    "\n",
    "#             # start decode shape 16,60\n",
    "#             start_decode = Variable(torch.LongTensor([[tag2index['<BOS>']]*batch_size])).cuda().transpose(1,0)\n",
    "#             start_decode = torch.cat((start_decode,tag_target[:,:-1]),dim=1)\n",
    "\n",
    "#             # tag score shape 960,124,  tag target 16,60\n",
    "#             # intent scoer shape 16,22,  intent target shape 16\n",
    "#             tag_score, intent_score = decoder(start_decode,output,bert_mask==0,bert_subtoken_maskings=subtoken_mask)\n",
    "#             loss_1 = loss_function_1(tag_target.view(-1),tag_score)\n",
    "\n",
    "            \n",
    "#             loss_2 = loss_function_2(intent_target,intent_score)\n",
    "#             loss_1 = loss_function_1(tag_score,tag_target.view(-1))\n",
    "#             # loss_2 = loss_function_2(intent_score,intent_target)\n",
    "#             loss = loss_1+ loss_2\n",
    "#             loss_1es.append(loss_1.data.cpu().numpy() if USE_CUDA else loss_1.data.numpy()[0])\n",
    "#             loss_2es.append(loss_2.data.cpu().numpy() if USE_CUDA else loss_2.data.numpy()[0])\n",
    "#             losses.append(loss.data.cpu().numpy() if USE_CUDA else loss.data.numpy()[0])\n",
    "#             loss.backward()\n",
    "#             torch.nn.utils.clip_grad_norm_(encoder.parameters(), 0.5)\n",
    "#             torch.nn.utils.clip_grad_norm_(middle.parameters(), 0.5)\n",
    "#             torch.nn.utils.clip_grad_norm_(decoder.parameters(), 0.5)\n",
    "#             torch.nn.utils.clip_grad_norm_(bert_layer.parameters(), 0.5)\n",
    "#             enc_optim.step()\n",
    "#             mid_optim.step()\n",
    "#             dec_optim.step()\n",
    "#             ber_optim.step()\n",
    "#             #print(bert_tokens[0])\n",
    "#             #print(tag_target[0])\n",
    "#             id_precision.append(accuracy_score(intent_target.detach().cpu(),torch.argmax(intent_score,dim=1).detach().cpu()))\n",
    "#             pred_list,target_list=mask_important_tags(torch.argmax(tag_score,dim=1).view(batch_size,LENGTH),tag_target,x_mask)\n",
    "#             sf_f1.append(f1_score(pred_list,target_list,average=\"micro\",zero_division=0))\n",
    "\n",
    "#         postfix_dict[\"T_L\"] = round(float(np.mean(losses)),4)\n",
    "#         postfix_dict[\"SF_L\"] = round(float(np.mean(loss_1es)),4)\n",
    "#         postfix_dict[\"ID_L\"] = round(float(np.mean(loss_2es)),4)\n",
    "#         postfix_dict[\"SF_F1\"] = round(float(np.mean(sf_f1)),3)\n",
    "#         postfix_dict[\"ID_P\"] = round(float(np.mean(id_precision)),3)\n",
    "\n",
    "#         losses=[]\n",
    "#         loss_1es=[]\n",
    "#         loss_2es=[]\n",
    "#         sf_f1=[]\n",
    "#         id_precision=[]\n",
    "#         #scheduler.step()\n",
    "\n",
    "#         #### TEST\n",
    "#         encoder.eval() # set to test mode\n",
    "#         middle.eval()\n",
    "#         decoder.eval()\n",
    "#         bert_layer.eval()\n",
    "#         with torch.no_grad(): # to turn off gradients computation\n",
    "#             for i,(x,tag_target,intent_target,bert_tokens,bert_mask,bert_toktype,subtoken_mask,x_mask) in enumerate(test_data):\n",
    "#                 batch_size=tag_target.size(0)\n",
    "#                 encoder.zero_grad()\n",
    "#                 middle.zero_grad()\n",
    "#                 decoder.zero_grad()\n",
    "#                 bert_layer.zero_grad()\n",
    "#                 bert_hidden,bert_pooler = bert_layer(bert_info=(bert_tokens,bert_mask,bert_toktype))\n",
    "#                 encoder_output = encoder(bert_last_hidden=bert_hidden)\n",
    "#                 output = middle(encoder_output,bert_mask==0,training=True)\n",
    "#                 start_decode = Variable(torch.LongTensor([[tag2index['<BOS>']]*batch_size])).cuda().transpose(1,0)\n",
    "#                 tag_score, intent_score = decoder(start_decode,output,bert_mask==0,bert_subtoken_maskings=subtoken_mask,infer=True)\n",
    "#                 loss_1 = loss_function_1(tag_score,tag_target.view(-1))\n",
    "#                 loss_2 = loss_function_2(intent_score,intent_target)\n",
    "#                 loss = loss_1 + loss_2\n",
    "#                 loss_1es.append(loss_1.data.cpu().numpy() if USE_CUDA else loss_1.data.numpy()[0])\n",
    "#                 loss_2es.append(loss_2.data.cpu().numpy() if USE_CUDA else loss_2.data.numpy()[0])\n",
    "#                 losses.append(loss.data.cpu().numpy() if USE_CUDA else loss.data.numpy()[0])\n",
    "#                 id_precision.append(accuracy_score(intent_target.detach().cpu(),torch.argmax(intent_score,dim=1).detach().cpu()))\n",
    "#                 pred_list,target_list=mask_important_tags(torch.argmax(tag_score,dim=1).view(batch_size,LENGTH),tag_target,x_mask)\n",
    "#                 sf_f1.append(f1_score(pred_list,target_list,average=\"micro\",zero_division=0))\n",
    "\n",
    "#         postfix_dict[\"Te_L\"] = round(float(np.mean(losses)),4)\n",
    "#         postfix_dict[\"Te_SF_L\"] = round(float(np.mean(loss_1es)),4)\n",
    "#         postfix_dict[\"Te_ID_L\"] = round(float(np.mean(loss_2es)),4)\n",
    "#         postfix_dict[\"SF_F1\"] = round(float(np.mean(sf_f1)),4)\n",
    "#         postfix_dict[\"ID_P\"] = round(float(np.mean(id_precision)),4)\n",
    "\n",
    "#         #Save Best\n",
    "#         max_sf_f1 = max_sf_f1 if round(float(np.mean(sf_f1)),4)<=max_sf_f1 else round(float(np.mean(sf_f1)),4)\n",
    "#         max_id_prec = max_id_prec if round(float(np.mean(id_precision)),4)<=max_id_prec else round(float(np.mean(id_precision)),4)\n",
    "#         if max_sf_f1_both<=round(float(np.mean(sf_f1)),4) and max_id_prec_both<=round(float(np.mean(id_precision)),4):\n",
    "#             max_sf_f1_both=round(float(np.mean(sf_f1)),4)\n",
    "#             max_id_prec_both=round(float(np.mean(id_precision)),4)\n",
    "#             torch.save(bert_layer,f\"models/ctran{_fn}-bertlayer.pkl\")\n",
    "#             torch.save(encoder,f\"models/ctran{_fn}-encoder.pkl\")\n",
    "#             torch.save(middle,f\"models/ctran{_fn}-middle.pkl\")\n",
    "#             torch.save(decoder,f\"models/ctran{_fn}-decoder.pkl\")\n",
    "#         postfix_dict[\"Best_ID_P\"] = max_id_prec_both\n",
    "#         postfix_dict[\"Best_SF_F1\"] = max_sf_f1_both\n",
    "#         enc_scheduler.step()\n",
    "#         dec_scheduler.step()\n",
    "#         mid_scheduler.step()\n",
    "#         ber_scheduler.step()\n",
    "\n",
    "#         epoiter.set_postfix(postfix_dict)\n",
    "#         epoiter.update(1)\n",
    "#     print(f\"max single SF F1: {max_sf_f1}\")\n",
    "#     print(f\"max single ID PR: {max_id_prec}\")\n",
    "#     print(f\"max mutual SF:{max_sf_f1_both}  PR: {max_id_prec_both}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Test\n",
    "\n",
    "The following cells is for reviewing the performance of CTran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_577617/886895912.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f'models/model.pkl').state_dict())\n"
     ]
    }
   ],
   "source": [
    "# This cell reloads the best model during training from hard-drive.\n",
    "model.load_state_dict(torch.load(f'models/model.pkl').state_dict())\n",
    "\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "global clipindex\n",
    "clipindex=0\n",
    "def removepads(toks,clip=False):\n",
    "    global clipindex\n",
    "    result = toks.copy()\n",
    "    for i,t in enumerate(toks):\n",
    "        if t==\"<PAD>\":\n",
    "            result.remove(t)\n",
    "        elif t==\"<EOS>\":\n",
    "            result.remove(t)\n",
    "            if not clip:\n",
    "                clipindex=i\n",
    "    if clip:\n",
    "        result=result[:clipindex]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of model prediction on test dataset\n",
      "Sentence           :  show me first class flights from new york to miami round trip\n",
      "Tag Truth          :  O O B-class_type I-class_type O O B-fromloc.city_name I-fromloc.city_name O B-toloc.city_name B-round_trip I-round_trip\n",
      "Tag Prediction     :  O O O B-class_type I-class_type O O B-fromloc.city_name I-fromloc.city_name O B-toloc.city_name B-round_trip\n",
      "Intent Truth       :  atis_flight\n",
      "Intent Prediction  :  atis_flight\n"
     ]
    }
   ],
   "source": [
    "print(\"Example of model prediction on test dataset\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    index = random.choice(range(len(test)))\n",
    "    test_raw = test[index][0]\n",
    "    bert_tokens = test_toks['input_ids'][index].unsqueeze(0).cuda()\n",
    "    bert_mask = test_toks['attention_mask'][index].unsqueeze(0).cuda()\n",
    "    bert_toktype = test_toks['token_type_ids'][index].unsqueeze(0).cuda()\n",
    "    subtoken_mask = test_subtoken_mask[index].unsqueeze(0).cuda()\n",
    "    test_in = prepare_sequence(test_raw,word2index)\n",
    "    test_mask = Variable(torch.BoolTensor(tuple(map(lambda s: s ==0, test_in.data)))).cuda() if USE_CUDA else Variable(torch.ByteTensor(tuple(map(lambda s: s ==0, test_in.data)))).view(1,-1)\n",
    "    start_decode = Variable(torch.LongTensor([[word2index['<BOS>']]*1])).cuda().transpose(1,0) if USE_CUDA else Variable(torch.LongTensor([[word2index['<BOS>']]*1])).transpose(1,0)\n",
    "    test_raw = [removepads(test_raw)]\n",
    "\n",
    "    tag_score, intent_score = model(bert_tokens,bert_mask,bert_toktype,subtoken_mask,infer=True)\n",
    "\n",
    "    v,i = torch.max(tag_score,1)\n",
    "    print(\"Sentence           : \",*test_raw[0])\n",
    "    print(\"Tag Truth          : \", *test[index][1][:len(test_raw[0])])\n",
    "    print(\"Tag Prediction     : \",*(list(map(lambda ii:index2tag[ii],i.data.tolist()))[:len(test_raw[0])]))\n",
    "    v,i = torch.max(intent_score,1)\n",
    "    print(\"Intent Truth       : \", test[index][2])\n",
    "    print(\"Intent Prediction  : \",index2intent[i.data.tolist()[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances where model predicted intent wrong\n",
      "Sentence           :  show flight and prices kansas city to chicago on next wednesday arriving in chicago by 7 pm\n",
      "Tag Truth          :  O O O O B-fromloc.city_name I-fromloc.city_name O B-toloc.city_name O B-depart_date.date_relative B-depart_date.day_name O O B-toloc.city_name B-arrive_time.time_relative B-arrive_time.time I-arrive_time.time\n",
      "Tag Prediction     :  O O O O O B-fromloc.city_name I-fromloc.city_name O B-toloc.city_name O B-depart_date.date_relative B-depart_date.day_name O O B-toloc.city_name B-arrive_time.time_relative B-arrive_time.time\n",
      "Intent Truth       :  atis_flight#atis_airfare\n",
      "Intent Prediction  :  atis_airfare\n",
      "--------------------------------------\n",
      "Sentence           :  what day of the week do flights from nashville to tacoma fly on\n",
      "Tag Truth          :  O O O O O O O O B-fromloc.city_name O B-toloc.city_name O O\n",
      "Tag Prediction     :  O O O O O O O O O B-fromloc.city_name O B-toloc.city_name O\n",
      "Intent Truth       :  atis_day_name\n",
      "Intent Prediction  :  atis_flight\n",
      "--------------------------------------\n",
      "Sentence           :  what days of the week do flights from san jose to nashville fly on\n",
      "Tag Truth          :  O O O O O O O O B-fromloc.city_name I-fromloc.city_name O B-toloc.city_name O O\n",
      "Tag Prediction     :  O O O O O O O O O B-fromloc.city_name I-fromloc.city_name O B-toloc.city_name O\n",
      "Intent Truth       :  atis_day_name\n",
      "Intent Prediction  :  atis_flight\n",
      "--------------------------------------\n",
      "Sentence           :  does the airport at burbank have a flight that comes in from kansas city\n",
      "Tag Truth          :  O O O O B-toloc.city_name O O O O O O O B-fromloc.city_name I-fromloc.city_name\n",
      "Tag Prediction     :  O O O O O B-city_name O O O O O O O B-fromloc.city_name\n",
      "Intent Truth       :  atis_flight\n",
      "Intent Prediction  :  atis_airport\n",
      "--------------------------------------\n",
      "Sentence           :  show me the connecting flights between boston and denver and the types of aircraft used\n",
      "Tag Truth          :  O O O B-connect O O B-fromloc.city_name O B-toloc.city_name O O O O O O\n",
      "Tag Prediction     :  O O O O B-connect O O B-fromloc.city_name O B-toloc.city_name O O O O O\n",
      "Intent Truth       :  atis_flight\n",
      "Intent Prediction  :  atis_aircraft\n",
      "--------------------------------------\n",
      "Sentence           :  list the airfare for american airlines flight 19 from jfk to lax\n",
      "Tag Truth          :  O O O O B-airline_name I-airline_name O B-flight_number O B-fromloc.airport_code O B-toloc.airport_code\n",
      "Tag Prediction     :  O O O O O B-airline_name I-airline_name O B-flight_number O B-fromloc.airport_code O\n",
      "Intent Truth       :  atis_airfare#atis_flight\n",
      "Intent Prediction  :  atis_airfare\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence           :  i need a round trip flight from san diego to washington dc and the fares\n",
      "Tag Truth          :  O O O B-round_trip I-round_trip O O B-fromloc.city_name I-fromloc.city_name O B-toloc.city_name B-toloc.state_code O O O\n",
      "Tag Prediction     :  O O O O B-round_trip I-round_trip O O B-fromloc.city_name I-fromloc.city_name O B-toloc.city_name B-toloc.state_code O O\n",
      "Intent Truth       :  atis_flight#atis_airfare\n",
      "Intent Prediction  :  atis_flight\n",
      "--------------------------------------\n",
      "Sentence           :  i need a round trip from atlanta to washington dc and the fares leaving in the morning\n",
      "Tag Truth          :  O O O B-round_trip I-round_trip O B-fromloc.city_name O B-toloc.city_name B-toloc.state_code O O O O O O B-depart_time.period_of_day\n",
      "Tag Prediction     :  O O O O B-round_trip I-round_trip O B-fromloc.city_name O B-toloc.city_name B-toloc.state_code O O O O O O\n",
      "Intent Truth       :  atis_flight#atis_airfare\n",
      "Intent Prediction  :  atis_airfare\n",
      "--------------------------------------\n",
      "Sentence           :  i need a round trip from phoenix to washington dc and the fare leaving in the morning\n",
      "Tag Truth          :  O O O B-round_trip I-round_trip O B-fromloc.city_name O B-toloc.city_name B-toloc.state_code O O O O O O B-depart_time.period_of_day\n",
      "Tag Prediction     :  O O O O B-round_trip I-round_trip O B-fromloc.city_name O B-toloc.city_name B-toloc.state_code O O O O O O\n",
      "Intent Truth       :  atis_flight#atis_airfare\n",
      "Intent Prediction  :  atis_flight\n",
      "--------------------------------------\n",
      "Sentence           :  i need flight and airline information for a flight from denver to salt lake city on monday departing after 5 pm\n",
      "Tag Truth          :  O O O O O O O O O O B-fromloc.city_name O B-toloc.city_name I-toloc.city_name I-toloc.city_name O B-depart_date.day_name O B-depart_time.time_relative B-depart_time.time I-depart_time.time\n",
      "Tag Prediction     :  O O O O O O O O O O O B-fromloc.city_name O B-toloc.city_name I-toloc.city_name I-toloc.city_name O B-depart_date.day_name O B-depart_time.time_relative B-depart_time.time\n",
      "Intent Truth       :  atis_flight#atis_airline\n",
      "Intent Prediction  :  atis_airline\n",
      "--------------------------------------\n",
      "Sentence           :  i need flight and fare information for thursday departing prior to 9 am from oakland going to salt lake city\n",
      "Tag Truth          :  O O O O O O O B-depart_date.day_name O B-depart_time.time_relative I-depart_time.time_relative B-depart_time.time I-depart_time.time O B-fromloc.city_name O O B-toloc.city_name I-toloc.city_name I-toloc.city_name\n",
      "Tag Prediction     :  O O O O O O O O B-depart_date.day_name O B-depart_time.time_relative O B-depart_time.time I-depart_time.time O B-fromloc.city_name O O B-toloc.city_name I-toloc.city_name\n",
      "Intent Truth       :  atis_flight#atis_airfare\n",
      "Intent Prediction  :  atis_flight\n",
      "--------------------------------------\n",
      "Sentence           :  i need flight and fare information departing from oakland to salt lake city on thursday before 8 am\n",
      "Tag Truth          :  O O O O O O O O B-fromloc.city_name O B-toloc.city_name I-toloc.city_name I-toloc.city_name O B-depart_date.day_name B-depart_time.time_relative B-depart_time.time I-depart_time.time\n",
      "Tag Prediction     :  O O O O O O O O O B-fromloc.city_name O B-toloc.city_name I-toloc.city_name I-toloc.city_name O B-depart_date.day_name B-depart_time.time_relative B-depart_time.time\n",
      "Intent Truth       :  atis_flight#atis_airfare\n",
      "Intent Prediction  :  atis_flight\n",
      "--------------------------------------\n",
      "Sentence           :  i need flight numbers and airlines for flights departing from oakland to salt lake city on thursday departing before 8 am\n",
      "Tag Truth          :  O O O O O O O O O O B-fromloc.city_name O B-toloc.city_name I-toloc.city_name I-toloc.city_name O B-depart_date.day_name O B-depart_time.time_relative B-depart_time.time I-depart_time.time\n",
      "Tag Prediction     :  O O O O O O O O O O O B-fromloc.city_name O B-toloc.city_name I-toloc.city_name I-toloc.city_name O B-depart_date.day_name O B-depart_time.time_relative B-depart_time.time\n",
      "Intent Truth       :  atis_flight_no#atis_airline\n",
      "Intent Prediction  :  atis_flight_no\n",
      "--------------------------------------\n",
      "Sentence           :  give me the flights and fares for a trip to cleveland from miami on wednesday\n",
      "Tag Truth          :  O O O O O O O O O O B-toloc.city_name O B-fromloc.city_name O B-depart_date.day_name\n",
      "Tag Prediction     :  O O O O O O O O O O O B-toloc.city_name O B-fromloc.city_name O\n",
      "Intent Truth       :  atis_flight\n",
      "Intent Prediction  :  atis_flight#atis_airfare\n",
      "--------------------------------------\n",
      "Sentence           :  how many northwest flights leave st. paul\n",
      "Tag Truth          :  O O B-airline_name O O B-fromloc.city_name I-fromloc.city_name\n",
      "Tag Prediction     :  O O O B-airline_name O O B-fromloc.city_name\n",
      "Intent Truth       :  atis_flight\n",
      "Intent Prediction  :  atis_quantity\n",
      "--------------------------------------\n",
      "Sentence           :  how many northwest flights leave washington dc\n",
      "Tag Truth          :  O O B-airline_name O O B-fromloc.city_name B-fromloc.state_code\n",
      "Tag Prediction     :  O O O B-airline_name O O B-fromloc.city_name\n",
      "Intent Truth       :  atis_flight\n",
      "Intent Prediction  :  atis_quantity\n",
      "--------------------------------------\n",
      "Sentence           :  how many flights does northwest have leaving dulles\n",
      "Tag Truth          :  O O O O B-airline_name O O B-fromloc.airport_name\n",
      "Tag Prediction     :  O O O O O B-airline_name O O\n",
      "Intent Truth       :  atis_flight\n",
      "Intent Prediction  :  atis_quantity\n",
      "--------------------------------------\n",
      "Sentence           :  are snacks served on tower air\n",
      "Tag Truth          :  O B-meal_description O O B-airline_name I-airline_name\n",
      "Tag Prediction     :  O O O O O B-airline_name\n",
      "Intent Truth       :  atis_meal\n",
      "Intent Prediction  :  atis_ground_service\n",
      "--------------------------------------\n",
      "Sentence           :  how many flights does alaska airlines have to burbank\n",
      "Tag Truth          :  O O O O B-airline_name I-airline_name O O B-toloc.city_name\n",
      "Tag Prediction     :  O O O O O B-airline_name I-airline_name O O\n",
      "Intent Truth       :  atis_flight\n",
      "Intent Prediction  :  atis_quantity\n",
      "--------------------------------------\n",
      "Total instances of wrong intent prediction is  19\n"
     ]
    }
   ],
   "source": [
    "print(\"Instances where model predicted intent wrong\")\n",
    "model.eval()\n",
    "total_wrong_predicted_intents = 0\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test)):\n",
    "        index = i\n",
    "        test_raw = test[index][0]\n",
    "        bert_tokens = test_toks['input_ids'][index].unsqueeze(0).cuda()\n",
    "        bert_mask = test_toks['attention_mask'][index].unsqueeze(0).cuda()\n",
    "        bert_toktype = test_toks['token_type_ids'][index].unsqueeze(0).cuda()\n",
    "        subtoken_mask = test_subtoken_mask[index].unsqueeze(0).cuda()\n",
    "        test_in = prepare_sequence(test_raw,word2index)\n",
    "        test_mask = Variable(torch.BoolTensor(tuple(map(lambda s: s ==0, test_in.data)))).cuda() if USE_CUDA else Variable(torch.ByteTensor(tuple(map(lambda s: s ==0, test_in.data)))).view(1,-1)\n",
    "        # print(removepads(test_raw))\n",
    "        start_decode = Variable(torch.LongTensor([[word2index['<BOS>']]*1])).cuda().transpose(1,0) if USE_CUDA else Variable(torch.LongTensor([[word2index['<BOS>']]*1])).transpose(1,0)\n",
    "        test_raw = [removepads(test_raw)]\n",
    "        \n",
    "        tag_score, intent_score = model(bert_tokens,bert_mask,bert_toktype,subtoken_mask,infer=True)\n",
    "\n",
    "        v,i = torch.max(intent_score,1)\n",
    "        if test[index][2]!=index2intent[i.data.tolist()[0]]:\n",
    "            v,i = torch.max(tag_score,1)\n",
    "            print(\"Sentence           : \",*test_raw[0])\n",
    "            print(\"Tag Truth          : \", *test[index][1][:len(test_raw[0])])\n",
    "            print(\"Tag Prediction     : \",*list(map(lambda ii:index2tag[ii],i.data.tolist()))[:len(test_raw[0])])\n",
    "            v,i = torch.max(intent_score,1)\n",
    "            print(\"Intent Truth       : \", test[index][2])\n",
    "            print(\"Intent Prediction  : \",index2intent[i.data.tolist()[0]])\n",
    "            print(\"--------------------------------------\")\n",
    "            total_wrong_predicted_intents+=1\n",
    "\n",
    "print(\"Total instances of wrong intent prediction is \",total_wrong_predicted_intents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp",
   "language": "python",
   "name": "temp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
